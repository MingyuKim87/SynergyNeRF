<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SynergyNeRF </title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://mingyukim87.github.io/SynergyNeRF/img/1_Teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1783">
    <meta property="og:image:height" content="1619">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://github.com/MingyuKim87/SynergyNeRF">
    <meta property="og:title" content="Synergistic Integration of Coordinate Network and Tensorial Feature for Improving NeRFs from Sparse Inputs">
    <meta property="og:description" content="We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs. ">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Synergistic Integration of Coordinate Network and Tensorial Feature for Improving NeRFs from Sparse Inputs">
    <meta name="twitter:description" content="We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs. ">
    <meta name="twitter:image" content="https://mingyukim87.github.io/SynergyNeRF/img/1_Teaser.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ðŸªž&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- equation fontsize -->
    <style>
        /* This CSS will increase the font size of the equation */
        #math-container {
            font-size: 20px; /* Increase this value to make the equation larger */
        }
    </style>

    <!-- citation style -->
    <style>
        .scrollbox {
            width: 100%;
            overflow: hidden;
            padding: 5px;
        }
    
        .scrollbox:hover {
            overflow-x: auto;
        }
    
        #bibtex {
            background-color: #7c7c7c; /* Light grey background */
            color: white; /* White text color for better readability on dark background */
            text-align: left; /* Left align text */
            width: 100%; /* Make textarea full width of its container */
            height: auto; /* Adjust height to fit the content */
            min-height: 200px; /* Minimum height to ensure usability */
            padding: 5px; /* Add some padding inside the textarea */
            font-family: monospace; /* Monospace font for bibliographic entries */
            white-space: nowrap; /* Prevent text from wrapping */
            border: none; /* Remove border */
        }
    </style>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                Synergistic Integration of Coordinate Network and<br>Tensorial Feature for Improving NeRFs from Sparse Inputs<br>
                <small>
                    ICML 2024
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://mingyukim87.github.io/">
                              Mingyu Kim
                            </a>
                            <br>KAIST AI
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://ami.postech.ac.kr/members/kim-jun-seong/">
                              Jun-Seong Kim
                            </a>
                            <br>POSTECH EE
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://fbsqkd.github.io/">
                              Se-Young Yun<sup>â€ </sup>
                            </a>
                            <br>KAIST AI
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://wityworks.com/">
                              Jin-Hwa Kim<sup>â€ </sup>
                            </a>
                            <br>NAVER AI Lab
                        </td>
                    </tr>
                </table>
                <!-- Explanation for the corresponding author symbol -->
                <p style="text-align:center; margin-top:5px; font-size: 0.8em;">
                    (<sup>â€ </sup> indicates corresponding authors)
                </p>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <!-- <li>
                            <a href="https://arxiv.org/abs/2306.17723">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                       <li>
                           <a href="https://youtu.be/_XNsRxzaPjw">
                           <img src="./img/youtube_icon.png" height="60px">
                               <h4><strong>Video</strong></h4>
                           </a>
                       </li> -->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Shiny Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <li>
                            <a href="https://github.com/MingyuKim87/SynergyNeRF" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <!-- Image container starts here -->
                <div class="row">
                    <!-- First image with label 'hexplane' -->
                    <div class="col-md-5">
                        <img src="img/0_HexPlane.png" class="img-responsive" alt="Hexplane Representation">
                        <p class="text-center"><strong><a href="https://caoang327.github.io/HexPlane/" target="_blank">HexPlane</a></strong></p>
                    </div>
                    <!-- Arrow symbol -->
                    <div class="col-md-2 text-center">
                        <h1 style="font-size: 20px; line-height: 120px; vertical-align: middle;">â†’</h1>
                    </div>
                    <!-- Second image with label 'ours' -->
                    <div class="col-md-5">
                        <img src="img/0_Ours.png" class="img-responsive" alt="Our Method">
                        <p class="text-center"><strong>Ours</strong></p>
                    </div>
                </div>
                <!-- Image container ends -->

                <p class="text-justify">
                    In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals.
                    The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. 
                    We demonstrate that using residual connections between them seamlessly preserves their own inherent properties.
                    Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. 
                    We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs.                
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    TL;DR:
                </h3>
                <p class="text-justify">
                    We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs.
                    This technique consistently proves effective in both static and dynamic NeRF applications, outperforming existing methods.
                </p>
            </div>
        </div>

        <div class="row">
           <div class="col-md-8 col-md-offset-2">
               <h3>
                   Video
               </h3>
               <div class="text-center">
                   <!-- <div style="position:relative;padding-top:56.25%;">
                       <iframe src="https://www.youtube.com/embed/_XNsRxzaPjw" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                   </div> -->
                   <p class="text-justify">
                    TBD
                </p>
               </div>
           </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Residual Neural Radiance Fields Spanning Diverse Spectrum
                </h3>
                <div class="text-center">
                    <img src="./img/2_Overview.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    The residual connection enhances the network's efficiency in responding to input values, emphasizing the importance of coordinate networks. 
                    We utilize ReLU activation to promote a low-frequency spectral bias. 
                    Our proposed method handles both low and high-frequency information in two distinct contexts. 
                    When only the coordinate network is employed, the output is biased towards low frequencies, aiding global reasoning. 
                    However, engaging all features yields clear and detailed images.
                </div>
                <br>
                <!-- <div class="text-center" id="math-container"> -->
                <div class="text-center">
                    \[
                        \begin{aligned}
                        \phi_1(s_k, f_k) &= \textit{h}\big(W_1^2 \cdot \textit{h}(W_1^1 \cdot (s_k \oplus f_k) + b_1^1) + b_1^2\big) \\
                        \phi_2(s_k, f_k, \phi_1) &= \textit{h}(W_2^2 \cdot \textit{h}(W_2^1 \cdot (s_k \oplus f_k \oplus \phi_1(s_k, f_k)) + b_2^1) + b_2^2
                        \end{aligned}
                    \]
                </div>
                <br>
                <div class="text-justify">
                    The parameters \( \{W_l, b_l\}_{l=1}^L \) are the weights and biases of the \( l \)-layer MLP.
                    The subsequent process is defined in MLP \( \phi_l(\cdot) \), where \( l>2 \), contains one pair of weights and biases.
                    The residual concatenation of coordinates value \( s_k \) and multi-plane features \( f_k \) across the first two blocks.
                    We employ ReLU activation \( \textit{h} \) to lean toward low-frequency spectral bias. \( \oplus \) denotes the concatenation operation.
                </div>
               <!-- <div class="text-center"> -->
                   <!-- <video id="refdir" width="40%" playsinline autoplay loop muted> -->
                       <!-- <source src="video/reflection_animation.mp4" type="video/mp4" /> -->
                   <!-- </video> -->
               <!-- </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scheme
                </h3>
                <div class="text-center">
                    <img src="./img/3_Scheme.png" width="100%">
                </div>
                <br>
                <h4>
                    Curriculum Weighting Strategy
                </h4>
                <div class="text-justify">
                    It encounters challenges in severe ill-conditioned situations, such as heavy occlusion and rapid motion, as seen in the <i>drums</i> in the static NeRF and the <i>standup</i> in the dynamic NeRF. 
                    To alleviate this issue, we propose a curriculum weighting strategy for multi-plane encoding, aiming to manipulate the engagement of multi-plane features per training step. 
                    This approach trains the coordinate-based network first, followed by the subsequent training of multi-plane features. In this subsection, we denote \( t \) as the training iteration. 
                    Technically, we introduce a weighting factor denoted as \( \alpha(t) \) to control the degree of engagement of multi-plane features along multi-plane channel dimensions.
                </div>
                <br>
                <!-- <div class="text-center" id="math-container"> -->
                <div class="text-center">
                    \[
                        \begin{equation}
                        \gamma_{j}(t)                                         = 
                        \begin{cases}
                                    0                                       & \text{if } \alpha(t) \leq j \\
                                    \frac{1-\cos ((\alpha(t) - j) \pi)}{2}  & \text{if } 0 < \alpha(t) - j \leq 1 \\
                                    1                                       & \text{otherwise, } 
                        \end{cases}
                        \end{equation}
                    \]
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experimental Results: Static NeRF
                </h3>
                <div class="text-justify">
                    Our proposed multi-plane encoding technique can exclusively capture fine-grained details while maintaining global shape learned by coordinate features, leading to more robust novel view synthesis in sparse-input scenarios.
                    We trained all models with 8 views. Due to space limitations, we present the results using low-resolution movies. When viewed at full resolution, ours clearly demonstrate superior performance.
                </div>
                <br>
                <h4>
                    <a href="https://jiawei-yang.github.io/FreeNeRF/" target="_blank">FreeNeRF <span class="small-text">(CVPR2023)</span></a>
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/FreeNeRF_Lego.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Lego</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/FreeNeRF_Drums.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Drums</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/FreeNeRF_Ship.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Ship</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
                <h4>
                    <a href="https://apchenstu.github.io/TensoRF/" target="_blank">TensoRF <span class="small-text">(ECCV2022)</span></a>
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/TensoRF_Lego.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Lego</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/TensoRF_Drums.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Drums</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/TensoRF_Ship.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Ship</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
                <h4>
                    <a href="https://sarafridov.github.io/K-Planes/" target="_blank">K-Planes <span class="small-text">(CVPR2023)</span></a>
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/K_Planes_Lego.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Lego</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/K_Planes_Drums.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Drums</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/K_Planes_Ship.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Ship</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
                <h4>
                    Ours
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/SynergyNeRF_Lego.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Lego</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/SynergyNeRF_Drums.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Drums</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/SynergyNeRF_Ship.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Ship</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experimental Results: Dynamic NeRF
                </h3>
                <div class="text-justify">
                    Performance improvement is even more evident in the dynamic NeRFs. 
                    We trained all models with 25 views.
                </div>
                <br>
                <h4>
                    <a href="https://caoang327.github.io/HexPlane/" target="_blank">HexPlane <span class="small-text">(CVPR2023)</span></a>
                </h4>
                <table width="90%">
                    <tr>
                        <td align="left" valign="top" width="35%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/HexPlane_Bouncingballs.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Bouncingballs</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="35%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/HexPlane_Standup.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Standup</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
                <h4>
                    Ours
                </h4>
                <table width="90%">
                    <tr>
                        <td align="left" valign="top" width="35%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/SynergyNeRF_Bouncingballs.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Bouncingballs</strong></p>
                            </video>
                        </td>
                        <td align="left" valign="top" width="35%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/SynergyNeRF_Standup.mp4" type="video/mp4" />
                                <p class="text-center"><strong>Standup</strong></p>
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    More Results
                </h3>
                <br>
                <div class="text-justify">
                    We have provided additional results featuring high-resolution movies. This video apparently demonstrates the superiority of our method.
                    For static Neural Radiance Fields (NeRF), please visit <a href="https://youtu.be/joVaUFa3HCg" target="_blank">the provided link</a>. 
                    We also offer results for dynamic NeRF at <a href="https://youtube.com/shorts/t_pIXZRSk7g?feature=share" target="_blank">this link</a>.
                </div>
                <br>
            </div>
        </div>
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bibliography
                </h3>
                <div class="form-group col-md-10 col-md-offset-1 scrollbox">
                    <textarea id="bibtex" class="form-control" readonly>
@InProceedings{kim2024synergistic,
    author    = {Kim, Mingyu and Kim, Jun-Seong, Yun, Se-Young and Kim, Jin-Hwa},  
    title     = {Synergistic Integration of Coordinate Network and Tensorial Feature for Improving NeRFs from Sparse Inputs},  
    booktitle = {Proceedings of the 41th International Conference on Machine Learning},
    year      = {2024},
    series    = {Proceedings of Machine Learning Research},
    publisher = {PMLR},  
    }
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [No.2022-0-00641, XVoice: Multi-Modal Voice Meta Learning]. 
                    A portion of this work was carried out during an internship at <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab" target="_blank">NAVER AI Lab</a>.
                    We also extend our gratitude to <a href="https://actnova.io" target="_blank">ACTNOVA</a> for providing the computational resources required.
                </p>
            </div>
        </div>
    </div>


</body></html>
