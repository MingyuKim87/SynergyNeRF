<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SynergyNeRF </title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://mingyukim87.github.io/SynergyNeRF/img/1_Teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1783">
    <meta property="og:image:height" content="1619">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://github.com/MingyuKim87/SynergyNeRF">
    <meta property="og:title" content="Synergistic Integration of Coordinate Network and Tensorial Feature for Improving NeRFs from Sparse Inputs">
    <meta property="og:description" content="We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs. ">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Synergistic Integration of Coordinate Network and Tensorial Feature for Improving NeRFs from Sparse Inputs">
    <meta name="twitter:description" content="We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs. ">
    <meta name="twitter:image" content="https://mingyukim87.github.io/SynergyNeRF/img/1_Teaser.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ü™û&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                Synergistic Integration of Coordinate Network and<br>Tensorial Feature for Improving NeRFs from Sparse Inputs<br>
                <small>
                    ICML 2024
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://mingyukim87.github.io/">
                              Mingyu Kim
                            </a>
                            <br>KAIST AI
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://ami.postech.ac.kr/members/kim-jun-seong/">
                              Jun-Seong Kim
                            </a>
                            <br>POSTECH EE
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://fbsqkd.github.io/">
                              Se-Young Yun<sup>‚Ä†</sup>
                            </a>
                            <br>KAIST AI
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://wityworks.com/">
                              Jin-Hwa Kim<sup>‚Ä†</sup>
                            </a>
                            <br>NAVER AI Lab
                        </td>
                    </tr>
                </table>
                <!-- Explanation for the corresponding author symbol -->
                <p style="text-align:center; margin-top:5px; font-size: 0.8em;">
                    (<sup>‚Ä†</sup> indicates corresponding authors)
                </p>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <!-- <li>
                            <a href="https://arxiv.org/abs/2306.17723">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                       <li>
                           <a href="https://youtu.be/_XNsRxzaPjw">
                           <img src="./img/youtube_icon.png" height="60px">
                               <h4><strong>Video</strong></h4>
                           </a>
                       </li> -->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Shiny Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <li>
                            <a href="https://github.com/MingyuKim87/SynergyNeRF" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <!-- Image container starts here -->
                <div class="row">
                    <!-- First image with label 'hexplane' -->
                    <div class="col-md-5">
                        <img src="img/0_HexPlane.png" class="img-responsive" alt="Hexplane Representation">
                        <p class="text-center"><strong>Hexplane</strong></p>
                    </div>
                    <!-- Arrow symbol -->
                    <div class="col-md-2 text-center">
                        <h1 style="font-size: 20px; line-height: 120px; vertical-align: middle;">‚Üí</h1>
                    </div>
                    <!-- Second image with label 'ours' -->
                    <div class="col-md-5">
                        <img src="img/0_Ours.png" class="img-responsive" alt="Our Method">
                        <p class="text-center"><strong>Ours</strong></p>
                    </div>
                </div>
                <!-- Image container ends -->

                <p class="text-justify">
                    In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals.
                    The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. 
                    We demonstrate that using residual connections between them seamlessly preserves their own inherent properties.
                    Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. 
                    We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs.                
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    TL;DR:
                </h3>
                <p class="text-justify">
                    We incorporate multi-plane representation and coordinate networks to improve NeRFs from sparse-inputs.
                    This technique consistently proves effective in both static and dynamic NeRF applications, outperforming existing methods.
                </p>
            </div>
        </div>

        <div class="row">
           <div class="col-md-8 col-md-offset-2">
               <h3>
                   Video
               </h3>
               <div class="text-center">
                   <!-- <div style="position:relative;padding-top:56.25%;">
                       <iframe src="https://www.youtube.com/embed/_XNsRxzaPjw" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                   </div> -->
                   <p class="text-justify">
                    TBD
                </p>
               </div>
           </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Residual Neural Radiance Fields Spanning Diverse Spectrum
                </h3>
                <div class="text-center">
                    <img src="./img/2_Overview.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    The residual connection enhances the network's efficiency in responding to input values, emphasizing the importance of coordinate networks. 
                    We utilize ReLU activation to promote a low-frequency spectral bias. 
                    Our proposed method handles both low and high-frequency information in two distinct contexts. 
                    When only the coordinate network is employed, the output is biased towards low frequencies, aiding global reasoning. 
                    However, engaging all features yields clear and detailed images.
                </div>
                <br>
                <div class="text-center">
                    \[
                        \begin{aligned}
                        \phi_1(s_k, f_k) &= \textit{h}\big(W_1^2 \cdot \textit{h}(W_1^1 \cdot (s_k \oplus f_k) + b_1^1) + b_1^2\big) \\
                        \phi_2(s_k, f_k, \phi_1) &= \textit{h}(W_2^2 \cdot \textit{h}(W_2^1 \cdot (s_k \oplus f_k \oplus \phi_1(s_k, f_k)) + b_2^1) + b_2^2
                        \end{aligned}
                    \]
                </div>
                <br>
                <div class="text-justify">
                    The parameters \( \{W_l, b_l\}_{l=1}^L \) are the weights and biases of the \( l \)-layer MLP.
                    The subsequent process is defined in MLP \( \phi_l(\cdot) \), where \( l>2 \), contains one pair of weights and biases.
                    The residual concatenation of coordinates value $s_k$ and multi-plane features $f_k$ across the first two blocks.
                    We employ ReLU activation $\textit{h}$ to lean toward low-frequency spectral bias. 
                </div>
                
                
                <br>
                <div class="text-justify">
                    To generate the additional training rays based on the flipped reflection direction, we need a set of imaginary ray origins located in a suitable space considering the hitting point and the original input ray origins.
                    Since the vanilla NeRF models, which are trained with a dense set of images, tend to have the blending weight distribution whose peak is located on the point around the object surface p<sub>s</sub> = o + t<sub>s</sub>d, <i>i.e.,</i> the s-th sample whose blending weight is the highest along a ray, we place o‚Ä≤ so that the s-th sample of r‚Ä≤ is p<sub>s</sub>:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/frray_2.png" width="25%">
                </div>
                <br>
                <div class="text-justify">
                    resulting in our proposed flipped reflection ray, r‚Ä≤(t) = o‚Ä≤ + td‚Ä≤.
                </div>
                <br>
                <div class="text-justify">
                    However, since the estimated surface normals, which are used to derive the flipped reflection directions, are not the ground truth but the estimation, there exists a concern that even miscreated r‚Ä≤, which do not satisfy photo-consistency, can be used for training.
                    To address this problem, we mask the ineffective r‚Ä≤ by considering the angle Œ∏ between the estimated surface normals and reflection directions as follows:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/frray_3.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    where ‚àí(d&#770 ¬∑ n&#770) amounts to cosŒ∏ of original input rays and normal vectors, and œÑ indicates the threshold for filtering the invalid rays, which we set as 90<sup>‚ó¶</sup> unless specified.
                    Through this masking process, only r‚Ä≤ which are cast toward the photo-consistent point can be remained as we intend.
                </div>
<!--                <div class="text-center">-->
<!--                    <video id="refdir" width="40%" playsinline autoplay loop muted>-->
<!--                        <source src="video/reflection_animation.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--                </div>-->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Uncertainty-aware Regularization
                </h3>
                <div class="text-justify">
                    The naive application of existing regularization techniques with limited training views might not be consistently helpful across the different scenes due to the scene-by-scene different structure, resulting in overall performance degradation.
                    To address this problem, we propose <i>Uncertainty-aware Emptiness Loss (UE Loss)</i>, which reduces the floating artifacts consistently over the different scenes by considering the output uncertainty:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ueloss_1.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    œÅ amounts to the average of the summation of estimated scale parameters of RGB color distributions from all samples along a ray, which we use as the uncertainty of a ray.
                    By our proposed UE Loss, we are able to regularize the blending weights adaptively, i.e., the more uncertain a ray is, the more penalized the blending weights along the ray are.
                    It is able to reduce floating artifacts consistently across the scenes with different structures and enables to synthesize more reliable outputs by considering uncertainty.
                </div>
                <br>
                <div class="text-justify">
                    With our proposed UE Loss, ours improves both of the rendering quality and the reliability of the model outputs by a large margin compared to MixNeRF.
                </div>
                <div class="text-justify">
                    (From the left to the right: RGB, RGB_Std, Depth, Depth_Std)
                </div>
                <h4>
                    MixNeRF
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_rgbstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_depth.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix4" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_depthstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <h4>
                    FlipNeRF (Ours)
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip1" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip2" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_rgbstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip3" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_depth.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_depthstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bottleneck Feature Consistency
                </h3>
                <div class="text-justify">
                    We encourage the consistency of bottleneck feature distributions between r and r‚Ä≤, which are intermediate feature vectors, <i>i.e.,</i> outputs of the spatial MLP of NeRF, by Jensen-Shannon Divergence (JSD):
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/bfcloss_1.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    where œà(¬∑), b and b‚Ä≤ denote the softmax function, the bottleneck features of r and r‚Ä≤, respectively.
                    We regulate the pair of features effectively by enhancing consistency between bottleneck features without depending on additional feature extractors.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison with Baselines
                </h3>
                <div class="text-justify">
                    Our FlipNeRF estimates more accurate surface normals than other baselines, leading to the performance gain with better reconstructed fine details from limited input views.
                </div>
                <div class="text-justify">
                    (From the left to the right: mip-NeRF, Ref-NeRF, MixNeRF, FlipNeRF (Ours))
                </div>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mipnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb2" width="100%" playsinline autoplay loop muted>
                                <source src="video/refnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <br>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="normals1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mipnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals2" width="100%" playsinline autoplay loop muted>
                                <source src="video/refnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@InProceedings{Seo_2023_ICCV,
    author    = {Seo, Seunghyeon and Chang, Yeonjin and Kwak, Nojun},
    title     = {FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {22883-22893}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by NRF (2021R1A2C3006659) and IITP (2021-0-01343) both funded by Korean Government. It was also supported by Samsung Electronics (IO201223-08260-01).
                </p>
            </div>
        </div>
    </div>


</body></html>
